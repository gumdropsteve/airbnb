{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2c3ad2-f09b-433d-85f3-d2660c37b979",
   "metadata": {},
   "source": [
    "# Fixing Scrape.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd6686b-0a06-4d52-9d33-cc8143478149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import dask.delayed\n",
    "from time import sleep\n",
    "from dask import compute\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb33e0-048b-4524-b2bb-32fe27ab9f5f",
   "metadata": {},
   "source": [
    "We are going to start by running each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd648841-929c-4499-8489-6f3e9190d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"\n",
    "    returns a soup object that contains all the information of a given webpage\n",
    "    \n",
    "    response type: tuple\n",
    "    \n",
    "    example response: (bs4.element.ResultSet, '2023-10-08 22:35:37.830176')\n",
    "    \"\"\"\n",
    "    tos = str(datetime.now()) \n",
    "    result = requests.get(url)\n",
    "    content = result.content\n",
    "    page = BeautifulSoup(content, features='html')\n",
    "    return page, tos\n",
    "\n",
    "location = 'Japan'\n",
    "a_url = f'http://www.airbnb.com/s/{location}/homes'\n",
    "\n",
    "# get_page(a_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d333c8de-14b1-42a0-8323-88d68c9dce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = requests.get(a_url)\n",
    "\n",
    "type(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf730fdd-77b3-4021-9d6b-9c0cf0d6e3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(BeautifulSoup(result.content, features='html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "44213ec7-880e-4bb8-bdaf-9e23866353c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_page = get_page(a_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7f640519-a00b-496a-b60c-1c7f9b3a8ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56870f91-d749-4834-a60a-3e5a732b5927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a_page[0].findAll('div')) # , {'class':'_8ssblpx'}  #  _8ssblpx _uhpzdny _gig1e7 _1wcpzyga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5d9647f-6622-4237-b45b-ed1f2a547c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_page[0].findAll('div'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a2006513-0d20-49c3-9a09-59e121bfbcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_page[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4852b27e-72a2-48d5-8e43-ef3464db0427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-10-08 23:29:57.478695'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_page[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4252d454-d966-474d-a3a2-a74bb8454175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce022e09-c372-48f1-8b35-5e2537e90afa",
   "metadata": {},
   "source": [
    "One way it appears we can find all the listings is by finding everywhere where `target=\"listing_` exists. The part after `listing_` is the listing number, which is also a way to find the URL of the listing. For example, `44182350` can be found `https://www.airbnb.com/rooms/44182350`.\n",
    "\n",
    "It looks like this `get_room_classes()` function was meant to pull each of the image + text boxes from a page on Airbnb. Do we need to do that? Or do we just need the URLs? A big reason I ask this is the function depends on a `.findAll()` method call from a `soup_page` object which uses parameters `'div', {'class':'_8ssblpx'}`. Past `div` breaking, `_8ssblpx` feels almost certain to break.\n",
    "\n",
    "The functions below `get_room_classes()` extract specific information. What was I extracting from these listing boxes? It looks like almost everything...\n",
    "- listing url\n",
    "- listing title\n",
    "- top row (what_it_is, where_it_is)\n",
    "- room info\n",
    "- room price\n",
    "- room rating and number of ratings\n",
    "\n",
    "This is all information which can be collected from the page of the listing as well. Let's migrate it to there. It looks like the `class` below, which calls all these functions, can also be cleaned up so I think let's just rework everything and make sure the data coming out is the same format or we can reformat old data.\n",
    "\n",
    "To start with the fix, let's make a function, `get_listing_urls()` which collects all the listing URLs from the browse page. It could be interesting to get information like the display image chosen or other things which may only be available on the main browse pages from the browse page as well, but I am not going to worry about that now.\n",
    "\n",
    "First, I need to make sure the idea of collecting all the listing URLs works and figure out how to do it from the soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d42975d-30d5-4547-900f-717ef55cf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_room_classes(soup_page):\n",
    "#     \"\"\"\n",
    "#     returns all the listings that can be found on the page (soup object) in a list\n",
    "#     \"\"\"\n",
    "#     rooms = soup_page.findAll('div', {'class':'_8ssblpx'})  #  _8ssblpx _uhpzdny _gig1e7 _1wcpzyga\n",
    "#     result = []\n",
    "#     for room in rooms:\n",
    "#         result.append(room)\n",
    "#     return result\n",
    "\n",
    "\n",
    "# get_room_classes(get_page(a_url))\n",
    "# # AttributeError: 'tuple' object has no attribute 'findAll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97de7969-5266-49d3-86de-4adde156dd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a_page[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "011b33f6-ca5f-45b0-8dba-1587527f3821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_page[0].find_all(\"div\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb10418c-c8e6-4fa3-a646-93c81ec50de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_page[0].find_all(string=\"listing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee5974-8bf4-41ae-924f-f2e486791f8d",
   "metadata": {},
   "source": [
    "Trying to figure out how to find all the listings https://stackoverflow.com/questions/33396785/how-to-find-a-particular-word-in-html-page-through-beautiful-soup-in-python\n",
    "\n",
    "Finding everything that's like `aria-labelledby=\"tit` would work but finding all the `aria-labelledby` would be better then check which cointain \"listing_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d90f998-57f2-4e20-9690-227c35ff40fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the word \"sunny\" 4 times\n",
      "\n",
      "Whole content: \"today is a sunny day\"\n",
      "\tWord before: \"a\", word after: \"day\"\n",
      "Whole content: \"I love when it's sunny outside\"\n",
      "\tWord before: \"it's\", word after: \"outside\"\n",
      "Whole content: \"\n",
      "Call me sunny\n",
      "\"\n",
      "\tWord before: \"me\", word after: \"None\"\n",
      "Whole content: \"sunny is a cool word sunny\"\n",
      "\tWord before: \"None\", word after: \"is\"\n",
      "Whole content: \"sunny is a cool word sunny\"\n",
      "\tWord before: \"word\", word after: \"None\"\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import re\n",
    "\n",
    "data = '''\n",
    "<html>\n",
    "<body>\n",
    "<div>today is a sunny day</div>\n",
    "<div>I love when it's sunny outside</div>\n",
    "Call me sunny\n",
    "<div>sunny is a cool word sunny</div>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "searched_word = 'sunny'\n",
    "\n",
    "soup = bs4.BeautifulSoup(data, 'html.parser')\n",
    "results = soup.body.find_all(string=re.compile('.*{0}.*'.format(searched_word)), recursive=True)\n",
    "\n",
    "print('Found the word \"{0}\" {1} times\\n'.format(searched_word, len(results)))\n",
    "\n",
    "for content in results:\n",
    "    words = content.split()\n",
    "    for index, word in enumerate(words):\n",
    "        # If the content contains the search word twice or more this will fire for each occurence\n",
    "        if word == searched_word:\n",
    "            print('Whole content: \"{0}\"'.format(content))\n",
    "            before = None\n",
    "            after = None\n",
    "            # Check if it's a first word\n",
    "            if index != 0:\n",
    "                before = words[index-1]\n",
    "            # Check if it's a last word\n",
    "            if index != len(words)-1:\n",
    "                after = words[index+1]\n",
    "            print('\\tWord before: \"{0}\", word after: \"{1}\"'.format(before, after))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308f481-5bea-4d21-b3ac-f9ebc054441d",
   "metadata": {},
   "source": [
    "**I am concerned this method would be inefficient at scale.. but let's see if it can work on a single page to start.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99ee93f4-6e3c-4085-9e27-7eb01ae14bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searched_word = 'listing'\n",
    "\n",
    "# # len(a_page[0].find_all(string=re.compile('.*{0}.*'.format(searched_word)), recursive=True)[0])\n",
    "# type(a_page[0].find_all(string=re.compile('.*{0}.*'.format(searched_word)), recursive=True)[0])\n",
    "\n",
    "# for _ in a_page[0].find_all(string=re.compile('.*{0}.*'.format(searched_word)), recursive=True)[0].join(''):\n",
    "#     print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edfae3-8b77-49cf-941a-6635f673f9b0",
   "metadata": {},
   "source": [
    "This method is taking far too long to work at scale, or it broke...\n",
    "\n",
    "This may be more useful: https://stackoverflow.com/questions/52656353/get-specific-links-with-target-in-python-beautifulsoup\n",
    "\n",
    "It looks like if I can find all the tags, `a` in this example, then specify what I am looking for within each of those tags, I will be able to extract each bit I am after. What is the tag I am looking for?data-testid=\"card-container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187c3ce-0084-4230-b989-aea2af640244",
   "metadata": {},
   "outputs": [],
   "source": [
    "data-testid=\"card-container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c1f2891-ec5d-43ca-9a77-40b8ea0fbf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title_44182350'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_page[0].find('div', {'data-testid':'card-container'})['aria-labelledby']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f18cc02-bc3f-4060-b810-efa963687c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_44182350\n",
      "title_43127419\n",
      "title_15974146\n",
      "title_28142598\n",
      "title_24688711\n",
      "title_36286963\n",
      "title_5608532\n",
      "title_1298200\n",
      "title_875191656813979483\n",
      "title_52393892\n",
      "title_8487288\n",
      "title_745111324566410595\n",
      "title_39117472\n",
      "title_11701395\n",
      "title_41357662\n",
      "title_12770524\n",
      "title_9765116\n",
      "title_44637320\n"
     ]
    }
   ],
   "source": [
    "for _ in a_page[0].find_all('div', {'data-testid':'card-container'}):\n",
    "    print(_['aria-labelledby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "630abb66-cf22-4438-8925-1fc282d42db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['51113056',\n",
       " '48596594',\n",
       " '895088127637716421',\n",
       " '970046008971749390',\n",
       " '46318718',\n",
       " '581010866199809991',\n",
       " '9036683',\n",
       " '875687088662113784',\n",
       " '29113700',\n",
       " '9280298',\n",
       " '990798364955992889',\n",
       " '14832676',\n",
       " '976582464843609267',\n",
       " '30326048',\n",
       " '959070829847421670',\n",
       " '962616042346760815',\n",
       " '9813434',\n",
       " '21620323']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"\n",
    "    returns a soup object that contains all the information of a given webpage\n",
    "    \n",
    "    response type: tuple\n",
    "    \n",
    "    example response: (bs4.element.ResultSet, '2023-10-08 22:35:37.830176')\n",
    "    \"\"\"\n",
    "    tos = str(datetime.now()) \n",
    "    result = requests.get(url)\n",
    "    content = result.content\n",
    "    page = BeautifulSoup(content, features='html')\n",
    "    return page, tos\n",
    "\n",
    "location = 'Japan'\n",
    "a_url = f'http://www.airbnb.com/s/{location}/homes'\n",
    "\n",
    "a_page = get_page(a_url)\n",
    "\n",
    "def get_listing_ids_from_page(page):\n",
    "    listings = [_['aria-labelledby'].split('_')[1] for _ in page[0].find_all('div', {'data-testid':'card-container'})]\n",
    "    \n",
    "    return listings\n",
    "\n",
    "get_listing_ids_from_page(a_page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957123e-7a63-4684-8aff-9a245f45aa08",
   "metadata": {},
   "source": [
    "Great - now we have a way to get all the listing IDs from a page. Let's see if it works at scale....\n",
    "\n",
    "How do page URLs change??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d69ce3db-d5ee-4750-849f-80eb2fefb5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.airbnb.com/s/japan/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&query=Japan&place_id=ChIJLxl_1w9OZzQRRFJmfNR1QvU&flexible_trip_lengths%5B%5D=one_week&monthly_start_date=2023-11-01&monthly_length=3&price_filter_input_type=0&price_filter_num_nights=5&channel=EXPLORE&search_type=unknown&federated_search_session_id=6072c860-2732-4c6f-bb29-4699462829a0&pagination_search=true&cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0IjozNiwidmVyc2lvbiI6MX0%3D\n"
     ]
    }
   ],
   "source": [
    "page_3 = \"https://www.airbnb.com/s/japan/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&query=Japan&place_id=ChIJLxl_1w9OZzQRRFJmfNR1QvU&flexible_trip_lengths%5B%5D=one_week&monthly_start_date=2023-11-01&monthly_length=3&price_filter_input_type=0&price_filter_num_nights=5&channel=EXPLORE&search_type=unknown&federated_search_session_id=6072c860-2732-4c6f-bb29-4699462829a0&pagination_search=true&cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0IjozNiwidmVyc2lvbiI6MX0%3D\"\n",
    "print(page_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa65b1-245b-46e5-85d8-daf83dbc46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tab_id=home_tab&pagination_search=true&cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0IjozNiwidmVyc2lvbiI6MX0%3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9607770-47fe-462d-b6dc-3e22d4e3b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.airbnb.com/s/japan/homes?cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0IjozNiwidmVyc2lvbiI6MX0%3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e579c-5728-4923-92b5-2cad0245c2c8",
   "metadata": {},
   "source": [
    "#### Breaking down cursor\n",
    "The below URLs will take you to page 5 and page 6 of a search in Texas.. what is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68bbde4c-a330-4bd4-8233-b881d2ef607e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 129)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_5 = \"https://www.airbnb.com/s/Texas--United-States/homes?cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0Ijo3MiwidmVyc2lvbiI6MX0%3D\"\n",
    "url_6 = \"https://www.airbnb.com/s/Texas--United-States/homes?cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0Ijo5MCwidmVyc2lvbiI6MX0%3D\"\n",
    "\n",
    "len(url_5), len(url_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1f5e1aa-8714-49f8-b902-bbb3e14a484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_5 == url_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5054e5ad-4127-4678-92b0-3ffee399a2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_5) == len(url_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d123f8a-ccb1-41c4-853c-8d7e5a0f8a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('3i', '5C')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_5_diff = ''\n",
    "url_6_diff = ''\n",
    "\n",
    "for i in range(len(url_5)):\n",
    "\n",
    "    if url_5[i] == url_6[i]:\n",
    "        pass\n",
    "    else:\n",
    "        print(i)\n",
    "        url_5_diff += url_5[i]        \n",
    "        url_6_diff += url_6[i]     \n",
    "        \n",
    "        \n",
    "url_5_diff, url_6_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c0783fb7-3590-44c3-bac2-e55f1dce2191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('3i', '5C')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5 = \"cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0Ijo3MiwidmVyc2lvbiI6MX0%3D\"\n",
    "x6 = \"cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0Ijo5MCwidmVyc2lvbiI6MX0%3D\"\n",
    "\n",
    "url_5_diff = ''\n",
    "url_6_diff = ''\n",
    "\n",
    "for i in range(len(\"cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0Ijo3MiwidmVyc2lvbiI6MX0%3D\")):\n",
    "\n",
    "    if x5[i] == x6[i]:\n",
    "        pass\n",
    "    else:\n",
    "        print(i)\n",
    "        url_5_diff += x5[i]        \n",
    "        url_6_diff += x6[i]     \n",
    "        \n",
    "        \n",
    "url_5_diff, url_6_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "83e666d0-26f9-4830-ace7-baaeb079cc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3Mi', '5MC')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0Ijo5MCwidmVyc2lvbiI6MX0%3D\"[:53]\n",
    "\"cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0Ijo5MCwidmVyc2lvbiI6MX0%3D\"[53:]\n",
    "x5[54:57], x6[54:57]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a648f3f-70a6-43a8-8a19-776cd851264a",
   "metadata": {},
   "source": [
    "Alright, the difference between URLs for each page can be very minor and can be found in the `courser=` section.\n",
    "\n",
    "Here's page 15 for that search https://www.airbnb.com/s/Texas--United-States/homes?cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0IjoyNTIsInZlcnNpb24iOjF9\n",
    "\n",
    "And here's page 2 https://www.airbnb.com/s/Texas--United-States/homes?cursor=eyJzZWN0aW9uX29mZnNldCI6MSwiaXRlbXNfb2Zmc2V0IjoxOCwidmVyc2lvbiI6MX0%3D\n",
    "\n",
    "Both with everything but `coursor` removed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f1220-b39e-4659-9b66-9a22ba88ce74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0065f40-62f9-48b0-a044-715d5bb61f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56d968-638a-4a7f-b9da-4bc557cbd71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9fba7-dddf-4eba-920e-a3adda1ee6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971587e-e1d2-4e0f-bef1-b8584e821e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b40287-2167-4baa-8124-8a7efa436976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirBnbScrape:\n",
    "    \n",
    "    def __init__(self, location, location_alias):\n",
    "        \"\"\"\n",
    "        set location, base (url) link, and blank record books\n",
    "        \"\"\"\n",
    "        self.base_link = f'http://www.airbnb.com/s/{location}/homes'\n",
    "        self.location = location\n",
    "        self.location_alias = location_alias        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad32767-920a-415d-9c84-45d859392e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d5f08-c891-4f17-b2e6-a1def6520ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_listing_link(listing):\n",
    "    \"\"\"\n",
    "    returns the URL link of given listing\n",
    "    \"\"\"\n",
    "    listing_link = 'http://airbnb.com' + listing.find('a')['href']\n",
    "    listing_link = listing_link.split('?')[0]\n",
    "    return listing_link\n",
    "\n",
    "\n",
    "def get_listing_title(listing):\n",
    "    \"\"\"\n",
    "    returns the title of given listing\n",
    "    \"\"\"\n",
    "    title = listing.find('meta')['content']\n",
    "    title = title.split(' - null - ')\n",
    "    return title[0]\n",
    "\n",
    "\n",
    "def get_top_row(listing):\n",
    "    \"\"\"\n",
    "    returns the top row of given listing's info\n",
    "    \"\"\"\n",
    "    top_row = listing.find('div', {'class':'_1tanv1h'}).text  # _167gordg\n",
    "    top_row = top_row.split(' in ')\n",
    "    # what are we looking at?\n",
    "    what_it_is = top_row[0]\n",
    "    # where is it?\n",
    "    where_it_is = top_row[1]\n",
    "    return what_it_is, where_it_is\n",
    "\n",
    "\n",
    "def get_room_info(listing):\n",
    "    \"\"\"\n",
    "    returns room info of listing \n",
    "    \"\"\"\n",
    "    room_info = listing.find('div', {'class', '_kqh46o'}).text\n",
    "    split_info = [i.split() for i in room_info.split(' · ')]\n",
    "    room_dict = {}\n",
    "    for i in split_info:\n",
    "        if i not in [['Studio'], ['Half-bath']]:\n",
    "            if len(i) == 2:\n",
    "                room_dict[i[1]] = i[0]\n",
    "            # shared-baths\n",
    "            elif len(i) == 3:\n",
    "                i = [i[0], '-'.join([i[1], i[2]])]\n",
    "                room_dict[i[1]] = i[0]\n",
    "            else:\n",
    "                if i[1] == 'total':\n",
    "                    room_dict['bedrooms'] = [i[0]]\n",
    "                else:\n",
    "                    print(f'unexpected room_info | unexpected split_info len(i)=={len(i)}!=2!=3\\n{i}')\n",
    "                    room_dict[' '.join(i)] = i[0]\n",
    "        else:\n",
    "            # Half-baths and Studios\n",
    "            if i[0] == 'Studio':\n",
    "                room_dict['is_studio'] = True\n",
    "            room_dict[i[0]] = 0\n",
    "    \n",
    "    # need better solution for bedrooms\n",
    "    weird_bedrooms = 0 \n",
    "    try:\n",
    "        b = room_dict['bedrooms']\n",
    "        del b\n",
    "    except:\n",
    "        try:\n",
    "            room_dict['bedrooms'] = room_dict['bedroom']\n",
    "        except:\n",
    "            try:\n",
    "                room_dict['bedrooms'] = room_dict['Studio']\n",
    "            except:\n",
    "                weird_bedrooms += 1\n",
    "                print(f'weird bedrooms {weird_bedrooms}')\n",
    "                room_dict['bedrooms'] = room_dict.get('bedrooms')\n",
    "    \n",
    "    try:\n",
    "        room_dict['baths']\n",
    "    except:\n",
    "        try:\n",
    "            room_dict['baths'] = room_dict['bath']\n",
    "        except:\n",
    "            room_dict['baths'] = None\n",
    "    \n",
    "    room_dict['half_baths'] = room_dict.get('Half-bath')\n",
    "    room_dict['shared_baths'] = room_dict.get('shared-baths')\n",
    "    room_dict['is_studio'] = room_dict.get('is_studio', False)\n",
    "    room_dict['beds'] = room_dict.get('beds')\n",
    "    room_dict['guests'] = room_dict.get('beds')\n",
    "\n",
    "    # check for bedrooms list\n",
    "    if type(room_dict['bedrooms']) == list:\n",
    "        if len(room_dict['bedrooms']) == 1:\n",
    "            room_dict['bedrooms'] = float(room_dict['bedrooms'][0])\n",
    "        else:\n",
    "            raise Exception(f'unexpected bedrooms list | {room_dict[\"bedrooms\"]}')\n",
    "            \n",
    "    room_dict = {key:value for key,value in room_dict.items() if key in ['guests', 'bedrooms', 'beds', 'is_studio', 'baths', 'half_baths', 'shared_baths']}\n",
    "            \n",
    "    return room_dict\n",
    "\n",
    "\n",
    "def get_room_price(listing):\n",
    "    \"\"\"\n",
    "    returns the nightly rate (price) of given listing\n",
    "    \"\"\"\n",
    "    price_text = listing.find('div', {'class':'_ls0e43'}).text\n",
    "    price = price_text.split('$')\n",
    "    price = price[1]\n",
    "    # extract float value\n",
    "    price = price.split(\" \")[0]  # skip the $\n",
    "    # remove possible / at end of string\n",
    "    if '/' in price:\n",
    "        price = price[:len(price) - 1]\n",
    "    # adjust for places with > 999 reviews\n",
    "    if ',' in price:\n",
    "        price = ''.join(price.split(','))\n",
    "    return float(price)\n",
    "\n",
    "\n",
    "def get_room_rating_and_reviews(listing):\n",
    "    \"\"\"\n",
    "    returns star rating and number of reviews of given listing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = listing.find('span', {'class':'_18khxk1'}).text\n",
    "        output = output.split('\\xa0')\n",
    "        \n",
    "        avg_rating = float(output[0])\n",
    "        n_reviews = float(output[1][:-1].split('(')[1])\n",
    "\n",
    "        return avg_rating, n_reviews\n",
    "    except:\n",
    "        try:\n",
    "            return listing.find('span', {'class':'_18khxk1'}), listing.find('span', {'class':'_18khxk1'})\n",
    "        except:\n",
    "            raise Exception(f'get_room_rating_and_reviews | listing == {type(listing), len(listing)}')\n",
    "\n",
    "\n",
    "class airbnb_scrape():\n",
    "    \n",
    "    def __init__(self, location, location_alias):\n",
    "        \"\"\"\n",
    "        set location, base (url) link, and blank record books\n",
    "        \"\"\"\n",
    "        self.base_link = f'http://www.airbnb.com/s/{location}/homes'\n",
    "        self.location = location\n",
    "        self.location_alias = location_alias\n",
    "        \n",
    "        self.n_pages = None\n",
    "        self.n_results = None\n",
    "        self.page_urls = []\n",
    "        self.data_dir = 'data/'\n",
    "        \n",
    "        # set known basic amenities\n",
    "        self.possible = ['Gym', 'Wifi', 'Self check-in', 'Air conditioning', 'Pets allowed', 'Indoor fireplace', 'Hot tub', 'Free parking', 'Pool', 'Kitchen', 'Breakfast', 'Elevator', 'Washer', 'Dryer', \n",
    "                         'Heating', 'Waterfront', 'Dishwasher', 'Beachfront', 'Ski-in/Ski-out', 'Terrace', 'Sonos sound system', 'BBQ grill', 'Hair dryer', \"Chef's kitchen\", 'Wet bar', 'Sun loungers', \n",
    "                         'Home theater', 'Housekeeping', 'Gated property', 'Gas fireplace', 'Plunge pool', 'Infinity pool', 'Sun deck', 'Game room', 'Surround sound system', 'Resort access']\n",
    "\n",
    "        # set current schema column names\n",
    "        self.names = ['ds', 'search_filter', 'url', 'title', 'type', 'location', 'guests', 'bedrooms', 'beds', 'is_studio', 'baths', 'half_baths', 'shared_baths', 'price', 'avg_rating', 'n_reviews', 'gym_bool', \n",
    "                      'wifi_bool', 'self_check_in_bool', 'air_conditioning_bool', 'pets_allowed_bool', 'indoor_fireplace_bool', 'hot_tub_bool', 'free_parking_bool', 'pool_bool', 'kitchen_bool', 'breakfast_bool', \n",
    "                      'elevator_bool', 'washer_bool', 'dryer_bool', 'heating_bool', 'waterfront_bool', 'dishwasher_bool', 'beachfront_bool', 'ski_in_ski_out_bool', 'terrace_bool', 'sonos_sound_system_bool', \n",
    "                      'bbq_grill_bool', 'hair_dryer_bool', 'chefs_kitchen_bool', 'wet_bar_bool', 'sun_loungers_bool', 'home_theater_bool', 'housekeeping_bool', 'gated_property_bool', 'gas_fireplace_bool', \n",
    "                      'plunge_pool_bool', 'infinity_pool_bool', 'sun_deck_bool', 'game_room_bool', 'surround_sound_system_bool', 'resort_access_bool']\n",
    "        \n",
    "        self.dtypes = {'ds': 'object', 'search_filter': 'object', 'url': 'object', 'title': 'object', 'type': 'object', 'location': 'object', 'guests': 'float64', 'bedrooms': 'float64', 'beds': 'float64', \n",
    "                       'is_studio': 'bool', 'baths': 'float64', 'half_baths': 'float64', 'shared_baths': 'float64', 'price': 'float64', 'avg_rating': 'float64', 'n_reviews': 'float64', 'gym_bool': 'bool', \n",
    "                       'wifi_bool': 'bool', 'self_check_in_bool': 'bool', 'air_conditioning_bool': 'bool', 'pets_allowed_bool': 'bool', 'indoor_fireplace_bool': 'bool', 'hot_tub_bool': 'bool', 'free_parking_bool': \n",
    "                       'bool', 'pool_bool': 'bool', 'kitchen_bool': 'bool', 'breakfast_bool': 'bool', 'elevator_bool': 'bool', 'washer_bool': 'bool', 'dryer_bool': 'bool', 'heating_bool': 'bool', \n",
    "                       'waterfront_bool': 'bool', 'dishwasher_bool': 'bool', 'beachfront_bool': 'bool', 'ski_in_ski_out_bool': 'bool', 'terrace_bool': 'bool', 'sonos_sound_system_bool': 'bool', \n",
    "                       'bbq_grill_bool': 'bool', 'hair_dryer_bool': 'bool', 'chefs_kitchen_bool': 'bool', 'wet_bar_bool': 'bool', 'sun_loungers_bool': 'bool', 'home_theater_bool': 'bool', 'housekeeping_bool': 'bool', \n",
    "                       'gated_property_bool': 'bool', 'gas_fireplace_bool': 'bool', 'plunge_pool_bool': 'bool', 'infinity_pool_bool': 'bool', 'sun_deck_bool': 'bool', 'game_room_bool': 'bool', \n",
    "                       'surround_sound_system_bool': 'bool', 'resort_access_bool': 'bool'}\n",
    "\n",
    "    def get_basic_facilities(self, listing):\n",
    "        '''\n",
    "        returns a dictionary of the given listing's basic facilities with True / None values based on known possible basic facilites\n",
    "        '''\n",
    "        # make list of this listing's basic facilites\n",
    "        try:\n",
    "            basic_facilities = listing.findAll(\"div\", {\"class\":\"_kqh46o\"})[1].text\n",
    "            basic_facilities = basic_facilities.split(' · ')\n",
    "        except:\n",
    "            basic_facilities = []\n",
    "\n",
    "        # open a record for this listing\n",
    "        room_dict = {}\n",
    "        \n",
    "        # add each basic facility to this room's record \n",
    "        for f in basic_facilities:\n",
    "            if f in self.possible:\n",
    "                room_dict[f] = True\n",
    "            else:\n",
    "                # looks liek we have a new basic facility\n",
    "                i = input(f'unexpected basic_facilites | {f} | is new? (y/n) ')\n",
    "                if i == 'y':\n",
    "                    i = input(f'ok, new basic facility\\nwhat should the column name be?\\ne.g. Hot tub is hot_tub_bool\\n\"exit\" to quit\\n column name == ')\n",
    "                    if i != 'exit':\n",
    "                        # set new amenity\n",
    "                        room_dict[f] = True\n",
    "                        # update possible amenities and column names\n",
    "                        self.possible.append(f)\n",
    "                        self.names.append(i)\n",
    "                        print(f'\\nnew self.possible ==\\n{self.possible}\\n\\nnew self.names ==\\n{self.names}\\n\\nplease update now (sleeping 60 seconds)\\n')\n",
    "                        sleep(60)\n",
    "                    else:\n",
    "                        raise Exception(f\"not sure what's going on.. | unexpected basic_facilites | {f} | user exit\")\n",
    "                else:\n",
    "                    raise Exception(f\"not sure what's going on.. | unexpected basic_facilites | {f}\")\n",
    "        \n",
    "        # add None for any basic facilities this listing doesn't offer\n",
    "        for f in self.possible:\n",
    "            room_dict[f] = room_dict.get(f, None)\n",
    "        \n",
    "        return room_dict\n",
    "    \n",
    "    def find_n_results(self, soup_page):\n",
    "        \"\"\"\n",
    "        finds total number of search results from page 1 (of search results)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # keep track of how many results we have\n",
    "            self.n_results = soup_page.find('div', {'class':'_1h559tl'}).text\n",
    "        except:\n",
    "            raise Exception('n results not found on 1st page')\n",
    "    \n",
    "    def find_n_pages(self, soup_page, listings_per_page=20):\n",
    "        \"\"\"\n",
    "        finds number of existing pages from 1st page of search results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_results_string = soup_page.find('div', {'class':'_1h559tl'}).text \n",
    "            # check if 300+ club\n",
    "            if '300+' in n_results_string:\n",
    "                self.n_pages = 15\n",
    "            else:\n",
    "                split_results_string = n_results_string.split(' of ')\n",
    "                n_total_results_string = split_results_string[1]\n",
    "                # check for unknown + edge case\n",
    "                if '+' in n_total_results_string:\n",
    "                    raise Exception(f'+ in n_total_results_string but 300+ is not\\nn_total_results_string == {n_total_results_string}')\n",
    "                else:\n",
    "                    # find number of results\n",
    "                    split_total_results_string = n_total_results_string.split(' ')\n",
    "                    n_total_results = int(split_total_results_string[0])\n",
    "                    n_pages = n_total_results / listings_per_page \n",
    "                    n_pages = math.ceil(n_pages)\n",
    "                    self.n_pages = n_pages\n",
    "        except:\n",
    "            print(f'find_n_pages error | {self.location}')\n",
    "            self.n_pages = 1\n",
    "        # tell me how many pages there are\n",
    "        print(self.n_pages)\n",
    "    \n",
    "    def make_page_urls(self, base_page, n_pages='auto', listings_per_page=20):\n",
    "        \"\"\"\n",
    "        makes pages for search results (sets of 20)\n",
    "        \"\"\"\n",
    "        # reset page urls\n",
    "        self.page_urls = []\n",
    "        # if n_pages wasn't set\n",
    "        if n_pages == 'auto':\n",
    "            # find out how many pages there are\n",
    "            self.find_n_pages(base_page, listings_per_page=listings_per_page)\n",
    "        # items_offset is 1st filter (?) or after 1st filter (&)\n",
    "        if '?' not in base_page:\n",
    "            c = '?'\n",
    "        else:\n",
    "            c = '&'\n",
    "        # create page urls\n",
    "        for i in range(self.n_pages):\n",
    "            # 1st page alread done earlier\n",
    "            if i != 0:\n",
    "                url = f'{base_page}{c}items_offset={i * listings_per_page}'\n",
    "                self.page_urls.append(url)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def record_dataset(self, listings, tos, _filter):\n",
    "        \"\"\"\n",
    "        take scraped room classes and record their information to csv\n",
    "\n",
    "        tos: time of scrape\n",
    "            > str datetime.datetime.now()\n",
    "\n",
    "        _filter: filter applied to scrape\n",
    "            > str, None if no filter\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for l in listings:\n",
    "            # listing link\n",
    "            a = get_listing_link(l)\n",
    "            # listing title\n",
    "            b = get_listing_title(l)\n",
    "            # top row info\n",
    "            c, d = get_top_row(l)\n",
    "            # room info (beds, baths, etc..)\n",
    "            _ = get_room_info(l)\n",
    "            e, f, g, h, i, j, k = _['guests'], _['bedrooms'], _['beds'], _['is_studio'], _['baths'], _['half_baths'], _['shared_baths']\n",
    "            del _\n",
    "            # room nightly rate\n",
    "            m = get_room_price(l)\n",
    "            # room rating and n reviews\n",
    "            n, o = get_room_rating_and_reviews(l)\n",
    "            # basic facilites\n",
    "            _ = self.get_basic_facilities(l)\n",
    "            p = [_[bf] for bf in self.possible]\n",
    "            # list of all listing info\n",
    "            out = [_filter] + [a, b, c, d, e, f, g, h, i, j, k, m, n, o] + p\n",
    "            # add time of scrape to data as 1st datapoint (jan 15 2021)\n",
    "            out = [tos] + out\n",
    "            # add it to the data collection \n",
    "            data.append(out)\n",
    "        \n",
    "        # add this scrape to the location's existing dataset\n",
    "        try:\n",
    "            pd.concat([pd.read_parquet(f'{self.data_dir}{self.location_alias}.parquet'), \n",
    "                       pd.DataFrame(data, columns=self.names)], axis=0).to_parquet(f'{self.data_dir}{self.location_alias}.parquet', index=False)\n",
    "        # first time we've scraped this location, make a new dataset\n",
    "        except:\n",
    "            # check this is actually new so we don't accidenly overwrite existing data (delete 'y'# from the below line if you want to perform manual check, outherwise defaults to make new file)\n",
    "            i = 'y'#input(f'recording new location: {self.location_alias}? (y/n) ')\n",
    "            if i == 'y':\n",
    "                # make dataframe from scraped data, column names from __init__()\n",
    "                df = pd.DataFrame(data, columns=self.names)\n",
    "                # go through each column\n",
    "                for column in self.dtypes:\n",
    "                    # our bool data is scraped as True/None, we need True/False\n",
    "                    if 'bool' in column:\n",
    "                        # fill None values in bool column with False\n",
    "                        df[column] = df[column].fillna(False)\n",
    "                    # convert column to expected dtype for parquet\n",
    "                    df[column] = df[column].astype(self.dtypes[column])\n",
    "                # write new parquet file\n",
    "                df.to_parquet(f'{self.data_dir}{self.location_alias}.parquet', index=False)\n",
    "                del df  # free up space\n",
    "            else:\n",
    "                raise Exception(\"not recording a new location, what's going on?\")\n",
    "    \n",
    "    def scrape_search(self, base_link, search_alias, _filter, n_pages='auto', printout=False):\n",
    "        \"\"\"\n",
    "        record results of a given search link\n",
    "        \"\"\"        \n",
    "        # get 1st page\n",
    "        base_link_page_1, t = get_page(base_link)\n",
    "        \n",
    "        # record the 1st page\n",
    "        if printout:\n",
    "            print(self.record_dataset(get_room_classes(base_link_page_1), tos=t, _filter=_filter))\n",
    "        else:\n",
    "            self.record_dataset(get_room_classes(base_link_page_1), tos=t, _filter=_filter)\n",
    "        \n",
    "        # get urls for other pages \n",
    "        if n_pages=='auto':\n",
    "            self.make_page_urls(self.base_link, self.find_n_pages(base_link_page_1))\n",
    "        else:\n",
    "            self.make_page_urls(self.base_link, n_pages)        \n",
    "        \n",
    "        for url in self.page_urls:\n",
    "            if printout:\n",
    "                page, t = get_page(url)\n",
    "                print(self.record_dataset(get_room_classes(page), tos=t, _filter=_filter))\n",
    "            else:\n",
    "                page, t = get_page(url)\n",
    "                self.record_dataset(get_room_classes(page), tos=t, _filter=_filter)\n",
    "                \n",
    "        # output where we can find the file (relative path)\n",
    "        return f'{self.data_dir}{self.location_alias}.parquet'\n",
    "    \n",
    "    @dask.delayed\n",
    "    def scrape_types(self, printout=False):\n",
    "        \"\"\"\n",
    "        record data from a loacations results for each of the big 4 room type filters and for each of those with superhosts only filter applied (8 total)\n",
    "        \"\"\"\n",
    "        print(f'starting {self.location.split(\"--\")[0]} @ {self.base_link}')  # scrape all 4 room types (default and with superhost filter)\n",
    "        \n",
    "        today = str(date.today())\n",
    "        try:\n",
    "            last_date_recorded = pd.read_parquet(f'{self.data_dir}{self.location_alias}.parquet').ds.str.split()[-1:].values[0][0]\n",
    "        except:\n",
    "            last_date_recorded = None\n",
    "            \n",
    "        # check to make sure we haven't already recorded this place today\n",
    "        if last_date_recorded != today:\n",
    "            # default search\n",
    "            self.scrape_search(self.base_link, f'{self.location_alias}', _filter='', printout=printout)\n",
    "            self.scrape_search(f'{self.base_link}?superhost=true', f'{self.location_alias}_super_hosts', _filter='super_hosts', printout=printout)\n",
    "\n",
    "            # entire homes only\n",
    "            self.scrape_search(f'{self.base_link}?room_types[]=Entire home', f'{self.location_alias}_entire_homes', _filter='entire_homes', printout=printout) \n",
    "            self.scrape_search(f'{self.base_link}?room_types[]=Entire home&superhost=true', f'{self.location_alias}_entire_home_super_hosts', _filter='entire_home_super_hosts', printout=printout)\n",
    "\n",
    "            # hotes rooms only\n",
    "            self.scrape_search(f'{self.base_link}?room_types[]=Hotel room', f'{self.location_alias}_hotel_rooms', _filter='hotel_rooms', printout=printout)\n",
    "            self.scrape_search(f'{self.base_link}?room_types[]=Hotel room&superhost=true', f'{self.location_alias}_hotel_room_super_hosts', _filter='hotel_room_super_hosts', printout=printout)\n",
    "\n",
    "            # private rooms only\n",
    "            self.scrape_search(f'{self.base_link}?room_types[]=Private room', f'{self.location_alias}_private_rooms', _filter='private_rooms', printout=printout)\n",
    "            self.scrape_search(f'{self.base_link}?room_types[]=Shared room&superhost=true', f'{self.location_alias}_private_room_super_hosts', _filter='private_room_super_hosts', printout=printout)\n",
    "\n",
    "            # shared rooms only\n",
    "            self.scrape_search(f'{self.base_link}?room_types[]=Private room', f'{self.location_alias}_shared_rooms', _filter='shared_rooms', printout=printout)\n",
    "            self.scrape_search(f'{self.base_link}?room_types[]=Shared room&superhost=true', f'{self.location_alias}_shared_room_super_hosts', _filter='shared_room_super_hosts', printout=printout)\n",
    "        # we already recorded today\n",
    "        else:\n",
    "            print(f'{self.location.split(\"--\")[0]} already recorded today')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    from where_are_you_going import locations, location_aliases\n",
    "    \n",
    "    # start timer\n",
    "    start = time.time()\n",
    "\n",
    "    # add each delayed location to a collection for delayed (parallel) scrape\n",
    "    collection = []\n",
    "    for _ in range(len(locations)):\n",
    "        # make airbnb scrape class instance for this location\n",
    "        l = airbnb_scrape(location=locations[_], location_alias=location_aliases[_])\n",
    "        \n",
    "        # make delayed scrape_types() method for this location\n",
    "        delayed_scrape = dask.delayed(l.scrape_types)(l, printout=False)\n",
    "\n",
    "        collection.append(delayed_scrape)\n",
    "\n",
    "    # execute delayed scrapes\n",
    "    compute(*collection)\n",
    "\n",
    "    print(f'runtime: {time.time() - start}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
